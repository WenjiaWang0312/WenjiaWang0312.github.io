<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
    <title>Wenjia Wang</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
    <style>
        body {
            font-family: 'Rubik', -apple-system, BlinkMacSystemFont,
                'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue',
                sans-serif;
            font-size: calc(10px+0.33vw);
            -webkit-font-smoothing: antialiased;
            padding: 5vh 20vw;
            color: #121314
        }

        p {
            font-weight: 400
        }

        b {
            font-weight: bolder
        }

        a {
            color: #1367a7;
            text-decoration: none
        }

        a:hover:after {
            top: 0
        }

        .table-img {
            width: 30%;
            padding-top: 20px;
            float: left
        }

        .table-content {
            width: 70%;
            float: left;
            padding-bottom: 20px
        }

        .paper-img {
            width: 30%;
            float: left
        }

        @media screen and (max-device-width:700px) {
            body {
                padding: 5vh 5vw
            }

            .table-img {
                width: 30%;
                padding-top: 30px
            }

            .table-content {
                width: 70%
            }
        }
    </style>
</head>


<body>
    <div width="100%">
        <div class="table-img">
            <img src="./assets/profile.jpg" width="70%" style="max-width: 200px;" alt="Wenjia Wang">
        </div>
        <div class="table-content" style="font-size:1em; line-height: 1">
            <p style="font-weight:500; font-size:1.8em">Wenjia Wang (王文佳)<br></p>
            <p>Ph.D Student </p>
            <p>The University of Hong Kong (HKU)</p>
            <!-- <p style="margin-top: 1em"><strong>Email</strong>: wwj2022@connect.hku.hk</p> -->
            <p>
                <a href="mailto:wwj2022@connect.hku.hk" target="_blank">
                    <img width="40" height="40" src="https://img.icons8.com/fluency/96/circled-envelope.png" alt="Email"
                        class="icon"></a>
                <a href="https://github.com/WenjiaWang0312" target="_blank"><img width="40" height="40"
                        src="https://img.icons8.com/ios-glyphs/90/github.png" alt="Github" class="icon"></a>
                <a href="https://scholar.google.com/citations?user=cVWmlYQAAAAJ&hl=zh-CN&authuser=1"
                    target="_blank"><img width="40" height="40"
                        src="https://img.icons8.com/color/96/google-scholar--v3.png" alt="Scholar" class="icon"></a>

            </p>

        </div>
    </div>

    <div style="font-size:1.2em; line-height: 1.4; margin-bottom: 50px;">
        <p>I'm currently a Ph.D. student (from Jan.2023) in the Department of Computer Science of HKU,
            supervised by <a href="https://i.cs.hku.hk/~taku/" target="_blank">Prof. Taku Komura</a>. I am interested in
            Human Motion Capture and Synthesis.</p>

        <h1 id="Exprience">Experience</h1>
        <p>I got my B.Eng. from <b>NUAA</b> in 2016, majored in Aircraft Designing. Then I got my M.Sc. degree from
            <b>Tongji University</b> in 2019, majored in Mechanical Engineering.
            I worked in <b>Sensetime, Beijing</b> from 2019 to 2020 as a research intern in OCR group lead by <a
                href="https://scholar.google.com/citations?user=Dqjnn0gAAAAJ&hl=zh-CN" target="_blank
            ">Mr. Ding Liang</a>. From 2021 to Mar.2022, I worked as an algorithm researcher in Zoetrope Group (used
            belongs to X-Lab) in <b>Sensetime, Shenzhen</b> lead by <a
                href="https://scholar.google.com.hk/citations?user=jZH2IPYAAAAJ&hl=en" target="_
            _blank">Dr. Lei Yang</a>. I also worked as an algorithm researcher for a short time in <b>Tencent,
                Shanghai</b> in 2022. After that, I had an internship in <b>Shanghai AI Lab</b>.

        <p> My career plan is to become a master in computer animation.
            In my spare time, I enjoy fitness and boxing. I also have a fondness for strategy and tower defense video games.
    </div>

    <div>
        <h1 id="News">News</h1>
        <ul>
            <li>Aug.12 2023. Zolly is selected as oral.</li>
        </ul>

        <ul>
            <li>Jul.14 2023. Zolly is accepted to ICCV2023.</li>
        </ul>

        <ul>
            <li>Jan.2023. Get enrolled in HKU.</li>
        </ul>
    </div>

    <div>
        <h1 id="main publications">Main Publications</h1>
        <div style="font-size:1em; line-height: 1.2;">
            <div class="paper-img">
                <img src="./assets/publications/zolly.png" width="80%">
            </div>
            <div class="table-content">
                <b style="font-size:1.2em;">Zolly: Zoom Focal Length Correctly for Perspective-Distorted Human Mesh
                    Reconstruction</b>
                <p><b>Wenjia Wang</b>, Yongtao Ge, Haiyi Mei, Zhongang Cai, Qingping Sun, Yanjun Wang, Chunhua Shen,
                    Lei Yang&dagger;, Taku Komura (&dagger;: corresponding author)</p>
    
                <p><b>ICCV 2023 oral</b></p>
                <p>[<a href="https://wenjiawang0312.github.io/projects/zolly/" target="_blank">webpage</a>]
                    [<a href="https://wenjiawang0312.github.io/projects/zolly" target="_blank">code&data</a>]
                    [<a href="https://arxiv.org/abs/2303.13796" target="_blank">arxiv</a>]
                </p>
            </div>
            <div style="clear: both;"></div>
        </div>

        <p></p>

        <div style="font-size:1em; line-height: 1.2;">
            <div class="paper-img">
                <img src="./assets/publications/humman.png" width="80%">
            </div>
            <div class="table-content">
                <b style="font-size:1.2em;">HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling</b>
                <p>Zhongang Cai*, Daxuan Ren*, Ailing Zeng*, Zhengyu Lin*, Tao Yu*, <b>Wenjia Wang*</b>,
                    Xiangyu Fan, Yang Gao, Yifan Yu, Liang Pan, Fangzhou Hong, Mingyuan Zhang, Chen Change Loy,
                    Lei Yang&dagger;, Ziwei Liu&dagger; (*: equal contribution, &dagger;: corresponding author)</p>
                <p><b>ECCV 2022 oral</b></p>
                <p>[<a href="https://caizhongang.github.io/projects/HuMMan/" target="_blank">webpage</a>][<a
                        href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670549.pdf"
                        target="_blank">eccv pdf</a>]
                    [<a href="https://arxiv.org/abs/2204.13686" target="_blank">arxiv</a>]
                </p>
            </div>
            <div style="clear: both;"></div>
        </div>
    </div>



    <div>
        <h1 id="Projects">Projects</h1>

        <div style="font-size:1em; line-height: 1.2;">
            <div class="paper-img">
                <img src="./assets/mmhuman3d.gif" width="80%">
            </div>
            <div class="table-content">
                <b style="font-size:1.2em;">MMHuman3D</b>
                <p><a href="https://github.com/open-mmlab/mmhuman3d">MMHuman3D</a> is an open source
                    PyTorch-based codebase for the use of 3D human parametric models. It is a part of the <a
                        href="https://openmmlab.com/">OpenMMLab</a> project.
                    It open-sourced in 2021, Dec.
                    <b>I am one of the main contributors, have contributed more than 20K lines of codes.</b>
                <p> - Reproducing popular methods with a modular framework</p>
                <p>- Supporting various datasets with a unified data convention</p>
                <p>- Versatile visualization toolbox</p>

                </p>
            </div>
            <div style="clear: both;"></div>
        </div>
    </div>

    <p></p>

    <div>
        <h1 id="previous publications">Previous Publications</h1>


        <div style="font-size:1em; line-height: 1.2;">
            <div class="paper-img">
                <img src="./assets/publications/trans10k.jpg" width="80%">
            </div>
            <div class="table-content">
                <b style="font-size:1.2em;">Segmenting transparent object in the wild with transformer</b>
                <p>Enze Xie, <b>Wenjia Wang</b>, Wenhai Wang, Peize Sun, Hang Xu, Ding Liang, Ping Luo</p>
                <p><b>IJCAI 2021</b></p>
                <p>[<a href="https://github.com/xieenze/Trans2Seg" target="_blank">code&data</a>][<a
                        href="https://www.ijcai.org/proceedings/2021/0165.pdf" target="_blank">ijcai pdf</a>]
                    [<a href="https://arxiv.org/abs/2101.08461" target="_blank">arxiv</a>]
                </p>
            </div>
            <div style="clear: both;"></div>
        </div>

        <p></p>


        <div style="font-size:1em; line-height: 1.2;">
            <div class="paper-img">
                <img src="./assets/publications/textzoom.png" width="80%">
            </div>
            <div class="table-content">
                <b style="font-size:1.2em;">Scene text image super-resolution in the wild</b>
                <p><b>Wenjia Wang</b>, Enze Xie, Xuebo Liu, Wenhai Wang, Ding Liang, Chunhua Shen, Xiang Bai</p>
                <p><b>ECCV 2020</b></p>
                <p>[<a href="https://github.com/WenjiaWang0312/TextZoom" target="_blank">code&data</a>][<a
                        href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550647.pdf"
                        target="_blank">eccv pdf</a>]
                    [<a href="https://arxiv.org/abs/2005.03341" target="_blank">arxiv</a>]
                </p>
            </div>
            <div style="clear: both;"></div>
        </div>

        <p></p>

        <div style="font-size:1em; line-height: 1.2;">
            <div class="paper-img">
                <img src="./assets/publications/translab.jpg" width="80%">
            </div>
            <div class="table-content">
                <b style="font-size:1.2em;">Segmenting Transparent Objects in the Wild</b>
                <p>Enze Xie, <b>Wenjia Wang</b>, Wenhai Wang, Mingyu Ding, Chunhua Shen, Ping Luo</p>
                <p><b>ECCV 2020</b></p>
                <p>[<a href="https://github.com/xieenze/Segment_Transparent_Objects" target="_blank">code&data</a>][<a
                        href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580681.pdf"
                        target="_blank">eccv pdf</a>]
                    [<a href="https://arxiv.org/abs/2003.13948" target="_blank">arxiv</a>]
                </p>
            </div>
            <div style="clear: both;"></div>
        </div>

        <p></p>


    </div>


    <div>
        <h1 id="Academic Services">Academic Services</h1>
        <ul>
            <li>Reviewer for TIP2022, ICCV2023.</li>
        </ul>
    </div>

    <div>
        <h1 id="Friends">Friends</h1>
        Some of my co-authors and friends: <a href="https://scholar.google.com.hk/citations?user=jZH2IPYAAAAJ&hl=en"
            target="__blank">Lei Yang (Sensetime, SH AI Lab)</a>,
        <a href="https://xieenze.github.io/">Enze Xie (Huawei Noah's Ark Lab)</a>, <a
            href="https://caizhongang.github.io/">Zhongang Cai (NTU, Sensetime)</a>,
        <a href="https://cshen.github.io/">Prof. Chunhua Shen (ZJU CAD Lab)</a>, <a
            href="https://frank-zy-dou.github.io/">Zhiyang Dou (HKU)</a>,
        <a href="https://scholar.google.co.uk/citations?user=GStTsxAAAAAJ&hl=zh-CN">Jingbo Wang (CUHK, SH AI Lab)</a>.


    </div>

    <div>
        <h1 id="Vistors">Vistors</h1>
        <script type='text/javascript' id='clustrmaps'
            src='//cdn.clustrmaps.com/map_v2.js?cl=0e1633&w=600&t=n&d=pimbbw_X9yYTMBN_gZbf9Q8YrNgLQcX85oh9jxfNoN4&co=0b4975&cmo=3acc3a&cmn=ff5353&ct=cdd4d9'></script>
    </div>

</body>

</html>